{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mannat5649144/DataScience-GenAI-Submissions-/blob/main/Week%207%207_01_convolutional_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "ii34M4muJPEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.01 Convolutional Neural Networks\n",
        "This tutorial will start by walking through a custom-built CNN in PyTorch. We will be working with a chest x-ray set from Kaggle ([here](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia)) where we are predicting whether a paitent has pneumonia or not.\n",
        "\n",
        "To get the data we need an API key from Kaggle. Set up an account (if you haven't got one already) and then click on your avatar in the top right and \"Account\" from the dropdown menu. Scroll down the page and you'll find a button to \"Create New API Token\". This downloads an API key to your PC which you can upload here."
      ],
      "metadata": {
        "id": "X6oownATtXqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "LHG_Y_YFFeEF",
        "outputId": "ff49f628-3351-4dd5-f4f9-4d8ad9e4616a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e850e33b-7efd-4672-ada9-219cc5498b48\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e850e33b-7efd-4672-ada9-219cc5498b48\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to move this to a folder called kaggle as this is where Colab will look to find the API key. This has been written as Linux commands as we are doing this directly on the underlying (virtual) machine not within Python. Linux commands start with a \"!\" in Colab. Linux is out of scope for the module so we won't go into detail on the code."
      ],
      "metadata": {
        "id": "iBOOZx3Kuey-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "pU043sJiFjet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction\n",
        "Now you can download the data. The following can be used for any Kaggle dataset. When you are on the dataset page click the three vertical dots on the right and select \"Copy API command\". This gives you code that looks like below (but add a \"!\" at the start):"
      ],
      "metadata": {
        "id": "EPsEVPNyuoo4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "048YVK6TChZV"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This has added the data as a zip folder. We can unzip it and create a new folder (Linux again):"
      ],
      "metadata": {
        "id": "yddDe7JWvFEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip chest-xray-pneumonia.zip -d chest-xray-pneumonia"
      ],
      "metadata": {
        "id": "pf7pLBawF3e0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, Test and Validation Split\n",
        "This data comes in three sets, `train`, `val` (validation) and `test`. We have seen two of these before, but `val` is new.\n",
        "\n",
        "Because deep learning effectively uses _epochs_ as a hyperparameter, and the length of time we train the model will impact under- and over-fitting, we want to measure performance every $x$ number of epochs. As with hyperparameter tuning in the classical space, we don't want to use `test` to check perofmance as this is cheating and can cause _data leakage_ (the model being able to see the test data). We also don't want to use the training data as this can't measure overfitting (overfitting can only be measured on unseen data). We could do cross validation but this will be very expensive to do over many epochs (i.e. it will take ages). So instead we will split our data an extra time and create a `val` set. This will be used to measure performance over epochs, but also can be used to tune other hyperparameters.\n",
        "\n",
        "For ease we will create some variables to store the path to the directories:"
      ],
      "metadata": {
        "id": "NSE9XpymvNeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/chest-xray-pneumonia/chest_xray/train\"\n",
        "test_dir = \"/content/chest-xray-pneumonia/chest_xray/test\"\n",
        "val_dir = \"/content/chest-xray-pneumonia/chest_xray/val\""
      ],
      "metadata": {
        "id": "yOQLobhOZsI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "Now we can add a generator to prepare out data. This data will perform basic cleaning operations on the data and feed it into the model.\n",
        "\n",
        "The three tasks we will do are:\n",
        "* `ToTensor()` changes the data into tensor format as before.\n",
        "* `Resize()` changes the size of the images to something smaller (to make computations faster and easier).\n",
        "* `Normalize()` (sic) effectively normalises the data as we have done previously with min-max or standardisation.\n",
        "\n",
        "For images normalisation is slighltly different as we are working with RGB pixels, meaning each data item is a red score between 0 (no red) and 255 (red); a green score between 0 (no green) and 255 (green); and a blue score between 0 (no blue) and 255 (blue). This means if a pixel is stored as:\n",
        "* RGB = [0,255,0] the pixel colour will be green.\n",
        "* RGB = [255,0,0] the colour will be red.\n",
        "* RGB = [0,0,0] the colour will be white.\n",
        "* RGB = [255,255,255] the colour will be black.\n",
        "* RGB = [255,0,255] the colour will be purple.\n",
        "* And so on.\n",
        "\n",
        "Because we are not scaling based on mean or max values in our data, all our data has the same scale, we apply the same transformation to all the datasets.\n",
        "\n",
        "We do these transformations for each of our datasets (`train`, `val` and `test`) and then create a `DataLoader` to move the data into the model. First we split into batches of 64 items per batch. We _shuffle_ our training data, meaning we put the data into random order in case there are biases in how it has been saved that would then be present in our batches. `pin_memory` is used just to make things a little faster as the Colab environment is slightly underpowered."
      ],
      "metadata": {
        "id": "xbsVBVhIvSom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define data transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Resize((32, 32)),  # Resize images (adjust size as needed)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet normalization\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets and apply transfomrations\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=val_transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64  # Adjust batch size as needed (hyperparameter!)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "dFf8B5smp1MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "Now we can build our model.\n",
        "\n",
        "We will use some of the layers we saw in the lecture, plus a few others. To summarise the model we have, in this order:\n",
        "* A convolutional layer (`Conv2d`) working in 3 dimensions (RGB), with 16 filters it finds (patterns it looks for) which will be within 3x3 (3 pixels wide and 3 pixels deep) squares.\n",
        "* A batch normalisation layer (`BatchNorm2d`) to normalise based on the batch mean and standard deviation.\n",
        "* A pooling layer (`MaxPool2d`) that reduces the size of the ouputs by taking the maximum value only from each 2x2 square, jumping (stride) 2 pixels each time. We pad the outer layer of the image with an additional set of zeros so that we can ensure that patterns near the outside of the image are scanned.\n",
        "* Another convolutional layer with 16 3x3 filters, now working across the 16 outputs of the pooling layer.\n",
        "* Another normalisation layer.\n",
        "* Another pooling layer identical to before.\n",
        "* A flatten layer (`Flatten`) which converts the various matrices from the previous layer to a long vector (i.e. a long list of 576 values).\n",
        "* A linear layer (`Linear`) which is a fully connected neural network layer (where each layer is linear regression with ReLU activation). This takes us from 576 values to 24.\n",
        "* A dropout layer (`Dropout`) which randomly turns off neurons during training to prevent overfitting.\n",
        "* A final linear layer with sigmoid activation (i.e. logistic regression) to convert the previous outputs to binary classification."
      ],
      "metadata": {
        "id": "8S2rBMlQvpHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PneumoniaCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PneumoniaCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3) # conv layer with 16 3x3 filters\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # maxpool to 2x2 filters, pad with 0\n",
        "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3) # conv layer with 16 3x3 filters\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # maxpool to 2x2 filters, pad with 0\n",
        "        self.flatten = nn.Flatten() # flatten layer\n",
        "        self.fc1 = nn.Linear(576, 24) # linear layer with 24 neurons\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(24, 1) # binary classification so 1x neuron\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x)) # convolution with ReLU\n",
        "        x = self.bn1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x) # Use flatten layer\n",
        "        x = F.relu(self.fc1(x)) # linear layer with ReLU\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc2(x)) # Sigmoid for binary classification\n",
        "        return x\n",
        "\n",
        "cnn = PneumoniaCNN()\n",
        "cnn"
      ],
      "metadata": {
        "id": "jT9g_eoUq07y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete our model let's specify the loss function and optimiser. We will use a slightly different optimiser than before - [Adam](https://arxiv.org/abs/1412.6980). You can read more from the link if you want to. We will also start with a learning rate of 0.001 (0.1%) and will reduce this number by 0.0005 every epoch. This means that as the training goes on we will take smaller and smaller steps so we can be sure we don't miss the optimal model (the one with the lowest loss)."
      ],
      "metadata": {
        "id": "NS36RAtXbamM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimiser\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCELoss() # Binary Cross-Entropy Loss for binary classification\n",
        "optimiser = optim.Adam(cnn.parameters(), lr=0.001, weight_decay=0.0005) # Adam optimizer\n",
        "# we add a weight decay parameter to reduce the learning rate each epoch"
      ],
      "metadata": {
        "id": "X4ZbqKGsK3yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how many parameters this is:"
      ],
      "metadata": {
        "id": "P8Lmr1pVr5uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to device\n",
        "from torchsummary import summary\n",
        "\n",
        "# Move data to GPU to get summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn = cnn.to(device)\n",
        "\n",
        "summary(cnn, (3, 32, 32))"
      ],
      "metadata": {
        "id": "_3-C4ULMr_em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "~17k parameters to learn! We better get busy:\n",
        "\n",
        "We are going to apply _early stopping_, which is a new idea. Because we might overfit by running too many epochs, we will set up to autoamtically stop if the performance on validation doesn't improve after 5 epochs (the number 5 is effectively a hyperparameter we will call our _patience_). I.e. if epoch #6 give us a our best score, we then have until epoch #11 to get a better score or the training automatically stops."
      ],
      "metadata": {
        "id": "Agm_F6Ievs3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping parameters\n",
        "patience = 5 # how many times we accept no improvement in val performance before early stopping\n",
        "best_val_loss = float('inf') # keep track of the best loss value\n",
        "epochs_no_improve = 0 # start the count at 0\n",
        "\n",
        "# empty lists to store losses for visualisation at the end\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500  # Arbitrarily high number - early stopping will end before this\n",
        "\n",
        "# Run on GPU if avaialble\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# For loop for training\n",
        "for epoch in range(num_epochs):\n",
        "    cnn.train() # switch to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # add images and label in batch to the GPU\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float().unsqueeze(1)  # Convert labels to float and add dimension\n",
        "        cnn = cnn.to(device) # Pass model to GPU as well\n",
        "\n",
        "        optimiser.zero_grad() # reset the optimiser to start optimisation again\n",
        "\n",
        "        # run the images through the model\n",
        "        outputs = cnn(images)\n",
        "        loss = criterion(outputs, labels) # calculate loss\n",
        "        loss.backward() # backpropogate\n",
        "        optimiser.step() # update parameters\n",
        "        running_loss += loss.item() # keep track of loss for visualisation\n",
        "\n",
        "    # store losses for visualisation\n",
        "    train_losses.append(running_loss/len(train_loader)) # average loss\n",
        "\n",
        "    # Validation\n",
        "    cnn.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds = [] # holder for predictions as 0 or 1\n",
        "    all_labels = [] # compare with labels\n",
        "    with torch.no_grad(): # not update/backpropogation\n",
        "      for images, labels in val_loader:\n",
        "          # add images and label in batch to the GPU\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device).float().unsqueeze(1)\n",
        "          cnn = cnn.to(device) # Pass model as well\n",
        "\n",
        "          # run the images through the model\n",
        "          outputs = cnn(images)\n",
        "          loss = criterion(outputs, labels) # calculate loss\n",
        "          val_loss += loss.item() # keep track of loss for visualisation\n",
        "\n",
        "    # store losses and recall for visualisation\n",
        "    val_losses.append(val_loss/len(val_loader)) # average loss\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss: # is this better than previous record?\n",
        "        best_val_loss = val_loss # if so set as new best\n",
        "        epochs_no_improve = 0 # make the no improve counter zero again\n",
        "    else:\n",
        "        epochs_no_improve += 1 # otherwise increase the count by one\n",
        "\n",
        "    train_loss_rnd = round(running_loss / len(train_loader), 4) # avg train loss\n",
        "    val_loss_rnd = round(val_loss / len(val_loader), 4) # avg val loss\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss_rnd}, Validation Loss: {val_loss_rnd}\")\n",
        "\n",
        "\n",
        "    if epochs_no_improve == patience: # if epochs without improvement is at the patience level\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break # end the training\n"
      ],
      "metadata": {
        "id": "bRu41LfNtyEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "Let's visualise the results:"
      ],
      "metadata": {
        "id": "4Stqkqz8vyaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JCedkXjKcJJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss on the training set has steadily improved. On the validation set its a bit more \"bouncey\" but then that's not a big set of images. It looks like our best performance was around 2 or 6 epochs. (Note, your results may change if you run it due to how we randomly split the data and so on). Let's see how we do on test:"
      ],
      "metadata": {
        "id": "d3PkH6Mxv12T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test set\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "cnn.eval()\n",
        "test_loss = 0.0\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float().unsqueeze(1)\n",
        "        outputs = cnn(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        predicted_labels = (outputs > 0.5).float()  # Convert probabilities to binary predictions\n",
        "        correct_predictions += (predicted_labels == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "test_loss_rnd = round(test_loss / len(test_loader), 4)\n",
        "test_recall = round(recall_score(labels.cpu(), predicted_labels.cpu()), 4)\n",
        "\n",
        "print(f\"Test Loss: {test_loss_rnd}\")\n",
        "print(f\"Test Recall: {test_recall}\")"
      ],
      "metadata": {
        "id": "Fv-jJ8sJcRBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "98% recall and relatively small loss (cross entropy) - again this may change if you run again but should be around these numbers. Our model seems to do pretty well ... but, would it be better if we changed some of the architecture or the batch size? Feel free to play a bit more if you want to!"
      ],
      "metadata": {
        "id": "UXoMUScAwBdy"
      }
    }
  ]
}